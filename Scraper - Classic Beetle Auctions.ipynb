{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90f793ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to the Google Sheet\n",
      "Data has been written to vw_beetles_auction_listings.csv\n",
      "Script has finished.\n"
     ]
    }
   ],
   "source": [
    "# Background: I love VW Beetles and I love data!  I have a 1958 in my garage and this project is more of a passion for me.\n",
    "#             I found that pricing data for classic VW sales was either expensive, incomplete, or difficult to access.\n",
    "#             So, I decided that I would curate a pricing dataset myself for Beetles (Type-1's) of all years. \n",
    "#             It is updated several times each week and originates from hundreds of auction houses around the world. \n",
    "#             The data may not be exhaustive, but it's a good sample!\n",
    "#\n",
    "# Objective:  To create a dataset that I can use to generate analytical insights for the benefit of VW Beetle enthusiasts. Enjoy! \n",
    "\n",
    "# Let's import some libraries that we'll us throughout the workflow.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to cleanup and normalize the Mileage field.\n",
    "def process_mileage(mileage_text):\n",
    "    if 'TMU' in mileage_text:\n",
    "        return ''\n",
    "    mileage_text = mileage_text.replace(' mi', '')\n",
    "    if 'k mi' in mileage_text:\n",
    "        return mileage_text.replace('k mi', ',000')\n",
    "    if 'k km' in mileage_text:\n",
    "        match = re.search(r'\\((\\d+k)\\)', mileage_text)\n",
    "        if match:\n",
    "            return match.group(1).replace('k', ',000')\n",
    "        match = re.search(r'\\((\\d+)\\)', mileage_text)\n",
    "        if match:\n",
    "            return f\"{int(match.group(1)):,}\"\n",
    "    if 'k' in mileage_text:\n",
    "        return mileage_text.replace('k', ',000')\n",
    "    if mileage_text == 'N/A':\n",
    "        return ''\n",
    "    return mileage_text\n",
    "\n",
    "# Function to extract the year from the title\n",
    "def extract_year(title):\n",
    "    match = re.search(r'\\b\\d{4}\\b', title)\n",
    "    if match and match.group().isdigit():\n",
    "        return match.group()\n",
    "    return 'N/A'\n",
    "\n",
    "# Function to convert date to proper date format\n",
    "def process_date(date_text):\n",
    "    try:\n",
    "        return datetime.strptime(date_text, '%b %d, %Y').date()\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Function to extract data from a single page using HTML elements on the page to isolate the elements I wanted to scrape.\n",
    "def extract_data_from_page(soup):\n",
    "    data = []\n",
    "    listings_section = soup.find('div', class_='listings-table')\n",
    "    if not listings_section:\n",
    "        return data\n",
    "    \n",
    "    listings = listings_section.find_all('div', class_='border border-gray-300 rounded-lg shadow-lg h-full flex flex-col justify-between table:h-auto table:flex-row table:rounded-none table:border-none table:shadow-none table:px-4')\n",
    "    \n",
    "    for listing in listings:\n",
    "        title_tag = listing.find('a', class_='text-xl leading-5 font-medium table:text-secondary table:text-base flex-1')\n",
    "        \n",
    "        # Extract the price\n",
    "        price_tag_black = listing.find('div', class_='text-sm font-medium text-black')\n",
    "        price_tag_red = listing.find('div', class_='text-sm font-medium text-red-600')\n",
    "        \n",
    "        price_tag = price_tag_black if price_tag_black else price_tag_red\n",
    "        \n",
    "        # Extract the mileage\n",
    "        mileage_divs = listing.find_all('div', class_='flex items-center')\n",
    "        mileage_tag = next((div.get_text(strip=True) for div in mileage_divs if 'mi' in div.get_text(strip=True) or 'km' in div.get_text(strip=True)), 'N/A')\n",
    "        mileage_tag = process_mileage(mileage_tag)\n",
    "        \n",
    "        # Extract the location\n",
    "        location_div = listing.find('div', class_='hidden table:block')\n",
    "        location_tag = location_div.find('div', class_='flex gap-2 items-center text-gray-500').find('div').text.strip() if location_div else 'N/A'\n",
    "        \n",
    "        # Extract the condition\n",
    "        condition_div = listing.find('div', class_='uppercase font-semibold text-xs')\n",
    "        condition_tag = condition_div.find('abbr', class_='no-underline').text.strip() if condition_div else 'N/A'\n",
    "        \n",
    "        # Extract the status\n",
    "        status_classes = [\n",
    "            'border font-medium uppercase inline-block whitespace-nowrap text-black bg-gray-200 px-1 py-0.5 text-sm rounded',\n",
    "            'border font-medium uppercase inline-block whitespace-nowrap text-green-600 border-green-600 px-1 py-0.5 text-sm rounded',\n",
    "            'border font-medium uppercase inline-block whitespace-nowrap text-red-600 border-red-600 px-1 py-0.5 text-sm rounded',\n",
    "            'border font-medium uppercase inline-block whitespace-nowrap text-white bg-black border-black px-1 py-0.5 text-sm rounded',\n",
    "            'border font-medium uppercase inline-block whitespace-nowrap text-black bg-gray-200 px-1 py-0.5 text-sm rounded'\n",
    "        ]\n",
    "        status_tag = None\n",
    "        for status_class in status_classes:\n",
    "            status_tag = listing.find('div', class_=status_class)\n",
    "            if status_tag:\n",
    "                break\n",
    "        status_text = status_tag.text.strip() if status_tag else 'N/A'\n",
    "        \n",
    "        # Extract the Auctioneer\n",
    "        auctioneer_tag = listing.find('a', class_='hover:underline table:text-black table:font-medium')\n",
    "        \n",
    "        # Extract the Date\n",
    "        date_tag = listing.find('span', class_='table:text-black')\n",
    "        date_text = process_date(date_tag.text.strip()) if date_tag else None\n",
    "        \n",
    "        # Extract the Link\n",
    "        link = title_tag['href'] if title_tag else None\n",
    "        \n",
    "        # Extract the US dollar price\n",
    "        price_text = price_tag.text.strip() if price_tag else 'N/A'\n",
    "        us_dollar_price = re.search(r'\\$(\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?)', price_text)\n",
    "        us_dollar_price = us_dollar_price.group(0) if us_dollar_price else ''\n",
    "        \n",
    "        # Extract the year from the title\n",
    "        year = extract_year(title_tag.text.strip()) if title_tag else 'N/A'\n",
    "        \n",
    "        data.append({\n",
    "            'Title': title_tag.text.strip() if title_tag else 'N/A',\n",
    "            'Year': year,\n",
    "            'Price': us_dollar_price,\n",
    "            'Location': location_tag,\n",
    "            'Condition': condition_tag,\n",
    "            'Mileage': mileage_tag,\n",
    "            'Status': status_text,\n",
    "            'Auctioneer': auctioneer_tag.text.strip() if auctioneer_tag else 'N/A',\n",
    "            'Date': date_text,\n",
    "            'Link': link\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# Function to handle pagination\n",
    "def scrape_all_pages(base_url):\n",
    "    all_data = []\n",
    "    page = 1\n",
    "    visited_pages = set()\n",
    "    \n",
    "    while True:\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        data = extract_data_from_page(soup)\n",
    "        if not data:\n",
    "            break\n",
    "        all_data.extend(data)\n",
    "\n",
    "        next_button = soup.find('a', rel='next')\n",
    "        if not next_button:\n",
    "            break\n",
    "        \n",
    "        next_page_url = next_button['href']\n",
    "        \n",
    "        parsed_url = urlparse(next_page_url)\n",
    "        next_page_number = int(parse_qs(parsed_url.query).get('page', [None])[0])\n",
    "        \n",
    "        if next_page_number in visited_pages:\n",
    "            break\n",
    "\n",
    "        visited_pages.add(next_page_number)\n",
    "        page = next_page_number\n",
    "        \n",
    "    return all_data\n",
    "\n",
    "# Function to authenticate and write data to Google Sheets\n",
    "def write_to_google_sheets(data):\n",
    "    try:\n",
    "        # Authenticate using the vw-beetle-scraper-1a0bb7d8e2f9.json file\n",
    "        scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = ServiceAccountCredentials.from_json_keyfile_name('vw-beetle-scraper-1a0bb7d8e2f9.json', scope)\n",
    "        client = gspread.authorize(creds)\n",
    "        \n",
    "        # Open the Google Sheet (use your sheet name or ID)\n",
    "        sheet = client.open(\"VW Beetles Auction Listings\").sheet1\n",
    "        \n",
    "        # Clear the sheet before writing new data\n",
    "        sheet.clear()\n",
    "        \n",
    "        # Prepare the data\n",
    "        headers = data[0].keys()\n",
    "        rows = [list(row.values()) for row in data]\n",
    "        \n",
    "        # Write the headers and rows in a single batch\n",
    "        sheet.append_row(list(headers))\n",
    "        sheet.append_rows(rows)\n",
    "        \n",
    "        print(\"Data has been written to the Google Sheet\")\n",
    "    except gspread.exceptions.APIError as api_err:\n",
    "        print(f\"An API error occurred: {api_err}\")\n",
    "        print(f\"API Error details: {api_err.response.text}\")\n",
    "    except gspread.exceptions.GSpreadException as gs_err:\n",
    "        print(f\"A GSpread exception occurred: {gs_err}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while writing to Google Sheets: {e}\")\n",
    "\n",
    "# Function to write data to CSV\n",
    "def write_to_csv(data):\n",
    "    try:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv('vw_beetles_auction_listings.csv', index=False)\n",
    "        print(\"Data has been written to vw_beetles_auction_listings.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing to CSV: {e}\")\n",
    "\n",
    "# Main function to run the scraper and write data to Google Sheets and CSV\n",
    "def main():\n",
    "    base_url = 'https://www.classic.com/m/volkswagen/beetle/type-1/'\n",
    "    try:\n",
    "        all_data = scrape_all_pages(base_url)\n",
    "        if all_data:\n",
    "            write_to_google_sheets(all_data)\n",
    "            write_to_csv(all_data)\n",
    "        else:\n",
    "            print('No data extracted')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    print(\"Script has finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9943e399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
